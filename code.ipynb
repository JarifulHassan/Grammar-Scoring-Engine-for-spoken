{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9552e64e",
   "metadata": {},
   "source": [
    "## ANN and residual gpr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84bb2fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Residual GPR + ANN Results:\n",
      "  RMSE: 0.9460778952754572\n",
      "  R-squared: 0.33876169892740793\n"
     ]
    }
   ],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel as C\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Feature extraction (same as before)\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "    file_name = row['filename']\n",
    "    label = row['label']\n",
    "\n",
    "    audio_file_path = f'Dataset/audios/train/{file_name}'\n",
    "    audio, sr = librosa.load(audio_file_path, sr=None)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    mfcc_mean = np.mean(mfcc, axis=1)\n",
    "\n",
    "    spec_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
    "    spec_centroid_mean = np.mean(spec_centroid)\n",
    "\n",
    "    zcr = librosa.feature.zero_crossing_rate(audio)\n",
    "    zcr_mean = np.mean(zcr)\n",
    "\n",
    "    chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
    "    chroma_mean = np.mean(chroma, axis=1)\n",
    "\n",
    "    feature_vector = np.hstack((mfcc_mean, spec_centroid_mean, zcr_mean, chroma_mean))\n",
    "    features.append(feature_vector)\n",
    "    labels.append(label)\n",
    "\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Train ANN\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(0.001), loss='mse')\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=16, validation_split=0.2, verbose=0)\n",
    "\n",
    "# 3. Predict with ANN\n",
    "y_pred_ann_train = model.predict(X_train).flatten()\n",
    "y_pred_ann_test = model.predict(X_test).flatten()\n",
    "\n",
    "# 4. Compute residuals\n",
    "residuals = y_train - y_pred_ann_train\n",
    "\n",
    "# 5. Train GPR on residuals\n",
    "kernel = C(1.0) * RBF(length_scale=1.0) + WhiteKernel()\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, alpha=1e-4, normalize_y=True)\n",
    "gpr.fit(X_train, residuals)\n",
    "\n",
    "# 6. Predict residuals on test set\n",
    "residuals_pred = gpr.predict(X_test)\n",
    "\n",
    "# 7. Final prediction\n",
    "y_final_pred = y_pred_ann_test + residuals_pred\n",
    "\n",
    "# 8. Evaluate\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_final_pred))\n",
    "r2 = r2_score(y_test, y_final_pred)\n",
    "\n",
    "print(\"Residual GPR + ANN Results:\")\n",
    "print(f\"  RMSE: {rmse}\")\n",
    "print(f\"  R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a0bf50",
   "metadata": {},
   "source": [
    "## SVM+residual_gpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeba3b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual GPR + SVM Results:\n",
      "  RMSE: 0.8573752693807208\n",
      "  R-squared: 0.45694212604792006\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel as C\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 2. Train SVM\n",
    "svm = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# 3. Predict with SVM\n",
    "y_pred_svm_train = svm.predict(X_train)\n",
    "y_pred_svm_test = svm.predict(X_test)\n",
    "\n",
    "# 4. Compute residuals\n",
    "residuals = y_train - y_pred_svm_train\n",
    "\n",
    "# 5. Train GPR on residuals\n",
    "kernel = C(1.0) * RBF(length_scale=1.0) + WhiteKernel()\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, alpha=1e-4, normalize_y=True)\n",
    "gpr.fit(X_train, residuals)\n",
    "\n",
    "# 6. Predict residuals on test set\n",
    "residuals_pred = gpr.predict(X_test)\n",
    "\n",
    "# 7. Final prediction\n",
    "y_final_pred = y_pred_svm_test + residuals_pred\n",
    "\n",
    "# 8. Evaluate\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_final_pred))\n",
    "r2 = r2_score(y_test, y_final_pred)\n",
    "\n",
    "print(\"Residual GPR + SVM Results:\")\n",
    "print(f\"  RMSE: {rmse}\")\n",
    "print(f\"  R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a484ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and scaler saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained SVM model\n",
    "svm_model_filename = 'svm_model.pkl'\n",
    "joblib.dump(svm, svm_model_filename)\n",
    "\n",
    "# Save the trained GPR model\n",
    "gpr_model_filename = 'gpr_model.pkl'\n",
    "joblib.dump(gpr, gpr_model_filename)\n",
    "\n",
    "# Save the scaler used for feature scaling\n",
    "scaler_filename = 'scaler.pkl'\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "\n",
    "print(\"Models and scaler saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61feddae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test features shape: (204, 27)\n",
      "         filename  predicted_label\n",
      "0   audio_804.wav         4.042393\n",
      "1  audio_1028.wav         3.702302\n",
      "2   audio_865.wav         3.277618\n",
      "3   audio_774.wav         2.384411\n",
      "4  audio_1138.wav         4.150069\n",
      "Predictions saved in 'submission.csv'!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the test dataset\n",
    "test_df = pd.read_csv('Dataset/test.csv')  # Update with the correct path\n",
    "\n",
    "# Load the saved models\n",
    "svm_model = joblib.load('svm_model.pkl')\n",
    "gpr_model = joblib.load('gpr_model.pkl')\n",
    "\n",
    "# Load the scaler used during training\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "# Prepare lists to store features and filenames\n",
    "test_features = []\n",
    "test_filenames = []\n",
    "\n",
    "# Extract features from the test audio files\n",
    "for index, row in test_df.iterrows():\n",
    "    file_name = row['filename']\n",
    "    \n",
    "    # Load the audio file\n",
    "    audio_file_path = f'Dataset/audios/test/{file_name}'  # Update with correct path\n",
    "    audio, sr = librosa.load(audio_file_path, sr=None)\n",
    "\n",
    "    # Extract MFCC features (13 MFCC coefficients)\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    mfcc_mean = np.mean(mfcc, axis=1)  # Mean of each MFCC coefficient\n",
    "\n",
    "    # Extract Spectral Centroid\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
    "    spectral_centroid_mean = np.mean(spectral_centroid)\n",
    "\n",
    "    # Extract other features like zero-crossing rate, spectral roll-off, etc. (if applicable)\n",
    "    # Example: Zero-crossing rate\n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(y=audio)\n",
    "    zero_crossing_rate_mean = np.mean(zero_crossing_rate)\n",
    "\n",
    "    # Example: Chroma features\n",
    "    chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
    "    chroma_mean = np.mean(chroma, axis=1)\n",
    "\n",
    "    # Combine features into a single vector (total features might add up to 27)\n",
    "    feature_vector = np.hstack((mfcc_mean, spectral_centroid_mean, zero_crossing_rate_mean, chroma_mean))\n",
    "\n",
    "    # Append the feature vector and filename\n",
    "    test_features.append(feature_vector)\n",
    "    test_filenames.append(file_name)\n",
    "\n",
    "# Convert to numpy array\n",
    "test_features = np.array(test_features)\n",
    "\n",
    "# Check if the number of features is correct (27 features)\n",
    "print(f\"Test features shape: {test_features.shape}\")\n",
    "\n",
    "# Normalize the features using the same scaler as during training\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "# Make predictions using the SVM model\n",
    "svm_predictions = svm_model.predict(test_features_scaled)\n",
    "\n",
    "# Make residual predictions using the GPR model\n",
    "residuals_pred = gpr_model.predict(test_features_scaled)\n",
    "\n",
    "# Combine the SVM predictions and GPR residuals\n",
    "final_predictions = svm_predictions + residuals_pred\n",
    "\n",
    "# Convert predictions to DataFrame for submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'filename': test_filenames,\n",
    "    'predicted_label': final_predictions\n",
    "})\n",
    "\n",
    "# Check the first few predictions\n",
    "print(submission_df.head())\n",
    "\n",
    "# Save predictions in the required format for submission\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved in 'submission.csv'!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e13f7a9",
   "metadata": {},
   "source": [
    "### here got 0.632 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21affb6",
   "metadata": {},
   "source": [
    "## Recursive implementation \n",
    "🔁 What You're Proposing\n",
    "Let’s say you have your initial features X. You want to:\n",
    "\n",
    "Train SVR on X → y\n",
    "\n",
    "Predict y_pred_1\n",
    "\n",
    "Concatenate y_pred_1 to X → X' = [X, y_pred_1]\n",
    "\n",
    "Train GPR on X' → y\n",
    "\n",
    "Predict y_pred_2, then concatenate y_pred_2 to get X'' = [X, y_pred_1, y_pred_2]\n",
    "\n",
    "Repeat this for 5 steps — each time appending the previous prediction as a feature and retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9f0b2295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursive Feedback Prediction (5 layers):\n",
      "  RMSE: 0.9134751219086583\n",
      "  R-squared: 0.3835502816919447\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel as C\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Make a copy of the original data\n",
    "X_train_stack = X_train.copy()\n",
    "X_test_stack = X_test.copy()\n",
    "\n",
    "current_model = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "current_model.fit(X_train_stack, y_train)\n",
    "y_pred_train = current_model.predict(X_train_stack)\n",
    "y_pred_test = current_model.predict(X_test_stack)\n",
    "\n",
    "# Add prediction as a new feature\n",
    "X_train_stack = np.hstack((X_train_stack, y_pred_train.reshape(-1, 1)))\n",
    "X_test_stack = np.hstack((X_test_stack, y_pred_test.reshape(-1, 1)))\n",
    "\n",
    "n_iters = 4  # Already did SVR once, do 4 more\n",
    "\n",
    "for i in range(n_iters):\n",
    "    kernel = C(1.0) * RBF(length_scale=1.0) + WhiteKernel()\n",
    "    gpr = GaussianProcessRegressor(kernel=kernel, alpha=1e-4, normalize_y=True)\n",
    "    gpr.fit(X_train_stack, y_train)\n",
    "    \n",
    "    y_pred_train = gpr.predict(X_train_stack)\n",
    "    y_pred_test = gpr.predict(X_test_stack)\n",
    "    \n",
    "    # Add predictions to the stack\n",
    "    X_train_stack = np.hstack((X_train_stack, y_pred_train.reshape(-1, 1)))\n",
    "    X_test_stack = np.hstack((X_test_stack, y_pred_test.reshape(-1, 1)))\n",
    "\n",
    "# Final evaluation\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Recursive Feedback Prediction (5 layers):\")\n",
    "print(f\"  RMSE: {rmse}\")\n",
    "print(f\"  R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b15eb1",
   "metadata": {},
   "source": [
    "## not good  Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4d78c1",
   "metadata": {},
   "source": [
    "✅ Full Pipeline: SVM + Residual GPR → ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ae630b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "SVM + Residual GPR → ANN Results:\n",
      "  RMSE: 0.4770611642155844\n",
      "  R-squared: 0.8318672417250499\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel as C\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "# 2. Scaling and splitting\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. SVM + Residual GPR\n",
    "svm = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm_train = svm.predict(X_train)\n",
    "y_pred_svm_test = svm.predict(X_test)\n",
    "\n",
    "residuals = y_train - y_pred_svm_train\n",
    "kernel = C(1.0) * RBF(length_scale=1.0) + WhiteKernel()\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, alpha=1e-4, normalize_y=True)\n",
    "gpr.fit(X_train, residuals)\n",
    "\n",
    "residuals_pred = gpr.predict(X_test)\n",
    "svm_gpr_pred_test = y_pred_svm_test + residuals_pred\n",
    "\n",
    "# 4. ANN using SVM+GPR output as input\n",
    "X_ann_train = np.hstack([X_test, svm_gpr_pred_test.reshape(-1, 1)])\n",
    "X_ann_test = X_ann_train  # Using same features for simplicity\n",
    "y_ann_train = y_test  # true labels\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_ann_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer=Adam(0.001), loss='mse')\n",
    "model.fit(X_ann_train, y_ann_train, epochs=100, batch_size=16, validation_split=0.2, verbose=0)\n",
    "\n",
    "# 5. Predict with ANN\n",
    "y_ann_pred = model.predict(X_ann_test).flatten()\n",
    "\n",
    "# 6. Final evaluation\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_ann_pred))\n",
    "r2 = r2_score(y_test, y_ann_pred)\n",
    "\n",
    "print(\"SVM + Residual GPR → ANN Results:\")\n",
    "print(f\"  RMSE: {rmse}\")\n",
    "print(f\"  R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a7068",
   "metadata": {},
   "source": [
    "## the best model that i have got till now\n",
    "SVM + Residual GPR → ANN Results:\n",
    "* lets save it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aac525c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save models\n",
    "# Save SVM model\n",
    "joblib.dump(svm, 'svm_model.pkl')\n",
    "# Save GPR model\n",
    "joblib.dump(gpr, 'gpr_model.pkl')\n",
    "# Save ANN model\n",
    "model.save('ann_model.h5')\n",
    "\n",
    "print(\"Models saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4f82af24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test features shape: (204, 27)\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "         filename  predicted_label\n",
      "0   audio_804.wav         1.882749\n",
      "1  audio_1028.wav         3.698372\n",
      "2   audio_865.wav         2.504019\n",
      "3   audio_774.wav         1.914483\n",
      "4  audio_1138.wav         2.941926\n",
      "Predictions saved in 'submission.csv'!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "# Load the test dataset\n",
    "test_df = pd.read_csv('Dataset/test.csv')  # Update with the correct path\n",
    "\n",
    "# Load the saved models\n",
    "svm_model = joblib.load('svm_model.pkl')\n",
    "gpr_model = joblib.load('gpr_model.pkl')\n",
    "\n",
    "# Load the scaler used during training\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "# Load the pre-trained ANN model (ensure the loss function is passed explicitly)\n",
    "ann_model = load_model('ann_model.h5', custom_objects={'mse': losses.MeanSquaredError()})\n",
    "\n",
    "# Prepare lists to store features and filenames\n",
    "test_features = []\n",
    "test_filenames = []\n",
    "\n",
    "# Extract features from the test audio files\n",
    "for index, row in test_df.iterrows():\n",
    "    file_name = row['filename']\n",
    "    \n",
    "    # Load the audio file\n",
    "    audio_file_path = f'Dataset/audios/test/{file_name}'  # Update with the correct path\n",
    "    audio, sr = librosa.load(audio_file_path, sr=None)\n",
    "\n",
    "    # Extract MFCC features (13 MFCC coefficients)\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    mfcc_mean = np.mean(mfcc, axis=1)  # Mean of each MFCC coefficient\n",
    "\n",
    "    # Extract Spectral Centroid\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
    "    spectral_centroid_mean = np.mean(spectral_centroid)\n",
    "\n",
    "    # Extract other features like zero-crossing rate, spectral roll-off, etc. (if applicable)\n",
    "    # Example: Zero-crossing rate\n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(y=audio)\n",
    "    zero_crossing_rate_mean = np.mean(zero_crossing_rate)\n",
    "\n",
    "    # Example: Chroma features\n",
    "    chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
    "    chroma_mean = np.mean(chroma, axis=1)\n",
    "\n",
    "    # Combine features into a single vector (total features might add up to 27)\n",
    "    feature_vector = np.hstack((mfcc_mean, spectral_centroid_mean, zero_crossing_rate_mean, chroma_mean))\n",
    "\n",
    "    # Append the feature vector and filename\n",
    "    test_features.append(feature_vector)\n",
    "    test_filenames.append(file_name)\n",
    "\n",
    "# Convert to numpy array\n",
    "test_features = np.array(test_features)\n",
    "\n",
    "# Check if the number of features is correct (27 features)\n",
    "print(f\"Test features shape: {test_features.shape}\")\n",
    "\n",
    "# Normalize the features using the same scaler as during training\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "# Make predictions using the SVM model\n",
    "svm_predictions = svm_model.predict(test_features_scaled)\n",
    "\n",
    "# Make residual predictions using the GPR model\n",
    "residuals_pred = gpr_model.predict(test_features_scaled)\n",
    "\n",
    "# Combine the SVM predictions and GPR residuals\n",
    "svm_gpr_pred_test = svm_predictions + residuals_pred\n",
    "\n",
    "# Prepare the input data for the ANN model\n",
    "X_ann_test = np.hstack([test_features_scaled, svm_gpr_pred_test.reshape(-1, 1)])\n",
    "\n",
    "# Make predictions using the pre-trained ANN model\n",
    "ann_predictions = ann_model.predict(X_ann_test).flatten()\n",
    "\n",
    "# Convert predictions to DataFrame for submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'filename': test_filenames,\n",
    "    'predicted_label': ann_predictions\n",
    "})\n",
    "\n",
    "# Check the first few predictions\n",
    "print(submission_df.head())\n",
    "\n",
    "# Save predictions in the required format for submission\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved in 'submission.csv'!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90031843",
   "metadata": {},
   "source": [
    "## SVM->Ann+residual GPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "189cf0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "SVM + Residual GPR → ANN → Residual GPR Results:\n",
      "  RMSE: 0.294985932600357\n",
      "  R-squared: 0.9357154183806489\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel as C\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have features and labels loaded\n",
    "X = np.array(features)  # Features (make sure this is a 2D array)\n",
    "y = np.array(labels)    # Labels (make sure this is a 1D array)\n",
    "\n",
    "# 1. Scaling and splitting data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. SVM + Residual GPR\n",
    "svm = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training and test data\n",
    "y_pred_svm_train = svm.predict(X_train)\n",
    "y_pred_svm_test = svm.predict(X_test)\n",
    "\n",
    "# Calculate residuals (difference between true labels and SVM predictions)\n",
    "residuals = y_train - y_pred_svm_train\n",
    "\n",
    "# Create GPR model to model residuals\n",
    "kernel = C(1.0) * RBF(length_scale=1.0) + WhiteKernel()\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, alpha=1e-4, normalize_y=True)\n",
    "gpr.fit(X_train, residuals)\n",
    "\n",
    "# Predict residuals on the test set using GPR\n",
    "residuals_pred = gpr.predict(X_test)\n",
    "\n",
    "# Combine SVM predictions and GPR residual predictions for final output\n",
    "svm_gpr_pred_test = y_pred_svm_test + residuals_pred\n",
    "\n",
    "# 3. ANN using SVM+GPR predictions as input\n",
    "X_ann_train = np.hstack([X_test, svm_gpr_pred_test.reshape(-1, 1)])\n",
    "X_ann_test = X_ann_train  # Using same features for simplicity\n",
    "y_ann_train = y_test  # True labels for ANN training\n",
    "\n",
    "# Define and compile the ANN model\n",
    "ann_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_ann_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer=Adam(0.001), loss='mse')\n",
    "\n",
    "# Train the ANN model\n",
    "ann_model.fit(X_ann_train, y_ann_train, epochs=100, batch_size=16, validation_split=0.2, verbose=0)\n",
    "\n",
    "# 4. Predict with ANN\n",
    "y_ann_pred = ann_model.predict(X_ann_test).flatten()\n",
    "\n",
    "# 5. Apply Residual GPR on ANN predictions\n",
    "residuals_ann = y_ann_train - y_ann_pred\n",
    "gpr_residual_ann = GaussianProcessRegressor(kernel=kernel, alpha=1e-4, normalize_y=True)\n",
    "gpr_residual_ann.fit(X_ann_train, residuals_ann)\n",
    "\n",
    "# Predict residuals on ANN predictions\n",
    "residuals_pred_ann = gpr_residual_ann.predict(X_ann_test)\n",
    "\n",
    "# Final prediction: ANN predictions + residuals from GPR\n",
    "final_predictions = y_ann_pred + residuals_pred_ann\n",
    "\n",
    "# 6. Final evaluation\n",
    "rmse = np.sqrt(mean_squared_error(y_test, final_predictions))\n",
    "r2 = r2_score(y_test, final_predictions)\n",
    "\n",
    "print(\"SVM + Residual GPR → ANN → Residual GPR Results:\")\n",
    "print(f\"  RMSE: {rmse}\")\n",
    "print(f\"  R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1d606e",
   "metadata": {},
   "source": [
    "### best model till now \n",
    "*SVM + Residual GPR → ANN → Residual GPR Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "19a3744b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models have been saved!\n"
     ]
    }
   ],
   "source": [
    "# Save the models\n",
    "# Save the SVR model using joblib\n",
    "joblib.dump(svm, 'svm_model.pkl')\n",
    "\n",
    "# Save the GPR model using joblib\n",
    "joblib.dump(gpr, 'gpr_model.pkl')\n",
    "\n",
    "# Save the ANN model using Keras' .save() method\n",
    "ann_model.save('ann_model.h5')\n",
    "\n",
    "print(\"Models have been saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "88e05147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "Predictions saved in 'submission.csv'!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "# Load the test dataset\n",
    "test_df = pd.read_csv('Dataset/test.csv')  # Update with the correct path\n",
    "\n",
    "# Load the saved models\n",
    "svm_model = joblib.load('svm_model.pkl')\n",
    "gpr_model = joblib.load('gpr_model.pkl')\n",
    "\n",
    "# Load the scaler used during training\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "# Load the pre-trained ANN model (ensure the loss function is passed explicitly)\n",
    "ann_model = load_model('ann_model.h5', custom_objects={'mse': losses.MeanSquaredError()})\n",
    "\n",
    "# Prepare lists to store features and filenames\n",
    "test_features = []\n",
    "test_filenames = test_df['filename'].tolist()  # Extract filenames from test.csv\n",
    "\n",
    "# Extract features from the test audio files\n",
    "for file_name in test_filenames:\n",
    "    # Load the audio file\n",
    "    audio_file_path = f'Dataset/audios/test/{file_name}'  # Correct path to audio files\n",
    "    audio, sr = librosa.load(audio_file_path, sr=None)\n",
    "\n",
    "    # Extract MFCC features (13 MFCC coefficients)\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    mfcc_mean = np.mean(mfcc, axis=1)  # Mean of each MFCC coefficient\n",
    "\n",
    "    # Extract Spectral Centroid\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
    "    spectral_centroid_mean = np.mean(spectral_centroid)\n",
    "\n",
    "    # Extract other features like zero-crossing rate, spectral roll-off, etc.\n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(y=audio)\n",
    "    zero_crossing_rate_mean = np.mean(zero_crossing_rate)\n",
    "\n",
    "    # Example: Chroma features\n",
    "    chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
    "    chroma_mean = np.mean(chroma, axis=1)\n",
    "\n",
    "    # Combine features into a single vector (total features might add up to 27)\n",
    "    feature_vector = np.hstack((mfcc_mean, spectral_centroid_mean, zero_crossing_rate_mean, chroma_mean))\n",
    "\n",
    "    # Append the feature vector\n",
    "    test_features.append(feature_vector)\n",
    "\n",
    "# Convert to numpy array\n",
    "test_features = np.array(test_features)\n",
    "\n",
    "# Normalize the features using the same scaler as during training\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "# Make predictions using the SVM model\n",
    "svm_predictions = svm_model.predict(test_features_scaled)\n",
    "\n",
    "# Make residual predictions using the GPR model\n",
    "residuals_pred = gpr_model.predict(test_features_scaled)\n",
    "\n",
    "# Combine the SVM predictions and GPR residuals\n",
    "svm_gpr_pred_test = svm_predictions + residuals_pred\n",
    "\n",
    "# Prepare the input data for the ANN model\n",
    "X_ann_test = np.hstack([test_features_scaled, svm_gpr_pred_test.reshape(-1, 1)])\n",
    "\n",
    "# Make predictions using the pre-trained ANN model\n",
    "ann_predictions = ann_model.predict(X_ann_test).flatten()\n",
    "\n",
    "# Convert predictions to DataFrame for submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'filename': test_filenames,\n",
    "    'predicted_label': ann_predictions\n",
    "})\n",
    "\n",
    "# Save predictions in the required format for submission\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved in 'submission.csv'!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9cf7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
